import os
import tempfile
from typing import List, Dict, Any

import streamlit as st
# å¼•å…¥ WebBaseLoader
from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings


def build_knowledge_base(
    uploaded_files,
    urls: List[str],  # âœ… å…³é”®ä¿®æ­£ï¼šè¿™é‡Œå¿…é¡»æœ‰ urls å‚æ•°
    chunk_size: int,
    chunk_overlap: int,
) -> None:
    """
    ä»ä¸Šä¼ æ–‡ä»¶å’Œ URL æ„å»ºå‘é‡çŸ¥è¯†åº“
    """
    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        all_documents = []
        file_stats: List[Dict[str, Any]] = []
        
        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        total_items = len(uploaded_files) + len(urls)
        if total_items == 0:
            st.warning("âš ï¸ è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶æˆ–è¾“å…¥ä¸€ä¸ªç½‘å€")
            return

        current_item = 0

        # === A. å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ ===
        if uploaded_files:
            for uploaded_file in uploaded_files:
                current_item += 1
                progress = current_item / (total_items + 1)
                progress_bar.progress(progress)
                status_text.text(f"ğŸ“– æ­£åœ¨åŠ è½½æ–‡ä»¶: {uploaded_file.name}")

                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    temp_filepath = tmp_file.name
                
                try:
                    if uploaded_file.name.lower().endswith(".pdf"):
                        loader = PyPDFLoader(temp_filepath)
                    else:
                        loader = TextLoader(temp_filepath)
                    
                    docs = loader.load()
                    all_documents.extend(docs)
                    
                    file_stats.append({
                        "name": uploaded_file.name,
                        "type": "ğŸ“„ æ–‡ä»¶",
                        "chars": sum(len(d.page_content) for d in docs)
                    })
                except Exception as e:
                    st.error(f"âŒ æ–‡ä»¶ {uploaded_file.name} è¯»å–å¤±è´¥: {e}")
                finally:
                    if os.path.exists(temp_filepath):
                        os.unlink(temp_filepath)

        # === B. å¤„ç† URL (å¸¦æµè§ˆå™¨ä¼ªè£…) ===
        if urls:
            for url in urls:
                if not url.strip(): continue
                
                current_item += 1
                progress = current_item / (total_items + 1)
                progress_bar.progress(progress)
                status_text.text(f"ğŸŒ æ­£åœ¨çˆ¬å–ç½‘é¡µ: {url}")

                try:
                    # ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé˜²æ­¢ 403 é”™è¯¯
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36"
                    }
                    loader = WebBaseLoader(url, header_template=headers)
                    docs = loader.load()
                    all_documents.extend(docs)
                    
                    file_stats.append({
                        "name": url,
                        "type": "ğŸŒ ç½‘é¡µ",
                        "chars": sum(len(d.page_content) for d in docs)
                    })
                except Exception as e:
                    st.warning(f"âš ï¸ ç½‘é¡µçˆ¬å–å¤±è´¥ ({url}): {e}")
                    continue

        # === C. åˆ‡åˆ†ä¸å‘é‡åŒ– ===
        if not all_documents:
            progress_bar.empty()
            status_text.empty()
            st.error("âŒ æœªæå–åˆ°æœ‰æ•ˆæ–‡æœ¬")
            return

        status_text.text("âœ‚ï¸ æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        split_docs = text_splitter.split_documents(all_documents)

        status_text.text("ğŸ”¢ æ­£åœ¨ç”Ÿæˆå‘é‡ç´¢å¼•...")
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = FAISS.from_documents(split_docs, embeddings)

        st.session_state["vectorstore"] = vectorstore
        st.session_state["doc_stats"] = file_stats 
        
        progress_bar.empty()
        status_text.empty()
        st.success(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å…±åŒ…å« {len(file_stats)} ä¸ªæ•°æ®æºã€‚")

    except Exception as e:
        progress_bar.empty()
        status_text.empty()
        st.error(f"æ„å»ºè¿‡ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")