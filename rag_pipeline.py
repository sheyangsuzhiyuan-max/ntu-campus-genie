import os
import tempfile
from typing import List, Dict, Any, Optional

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

from config import (
    EMBEDDING_MODEL,
    DEFAULT_CHUNK_SIZE,
    DEFAULT_CHUNK_OVERLAP,
    DEFAULT_KNOWLEDGE_FILES,
)


def build_knowledge_base(
    uploaded_files=None,
    urls: Optional[List[str]] = None,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
    use_default_files: bool = False,
) -> None:
    """
    ä»ä¸Šä¼ æ–‡ä»¶ã€URL æˆ–é»˜è®¤æ–‡ä»¶æ„å»ºå‘é‡çŸ¥è¯†åº“

    Args:
        uploaded_files: Streamlit ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        urls: è¦çˆ¬å–çš„ URL åˆ—è¡¨
        chunk_size: æ–‡æ¡£åˆ‡åˆ†å¤§å°
        chunk_overlap: æ–‡æ¡£åˆ‡åˆ†é‡å 
        use_default_files: æ˜¯å¦ä½¿ç”¨ data/ ç›®å½•ä¸‹çš„é»˜è®¤æ–‡ä»¶
    """
    # åˆå§‹åŒ–å‚æ•°
    uploaded_files = uploaded_files or []
    urls = urls or []

    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        all_documents = []
        file_stats: List[Dict[str, Any]] = []

        # === åŠ è½½é»˜è®¤æ–‡ä»¶ ===
        default_files = []
        if use_default_files:
            for file_path in DEFAULT_KNOWLEDGE_FILES:
                if os.path.exists(file_path):
                    default_files.append(file_path)
                else:
                    st.warning(f"âš ï¸ é»˜è®¤æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        total_items = len(uploaded_files) + len(urls) + len(default_files)
        if total_items == 0:
            st.warning("âš ï¸ è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶ã€è¾“å…¥ä¸€ä¸ªç½‘å€æˆ–åŠ è½½é»˜è®¤çŸ¥è¯†åº“")
            return

        current_item = 0

        # === A0. å¤„ç†é»˜è®¤æ–‡ä»¶ ===
        if default_files:
            for file_path in default_files:
                current_item += 1
                progress = current_item / (total_items + 1)
                progress_bar.progress(progress)
                status_text.text(f"ğŸ“– Loading default file: {os.path.basename(file_path)}")

                try:
                    if file_path.lower().endswith(".pdf"):
                        loader = PyPDFLoader(file_path)
                    else:
                        loader = TextLoader(file_path, encoding="utf-8")

                    docs = loader.load()

                    # è°ƒè¯•ï¼šæ‰“å°åŠ è½½ç»“æœçš„ç±»å‹
                    # st.write(f"DEBUG: åŠ è½½ {os.path.basename(file_path)}, ç±»å‹: {type(docs)}, æ˜¯åˆ—è¡¨: {isinstance(docs, list)}")

                    # ç¡®ä¿ docs æ˜¯åˆ—è¡¨ä¸”æ‰€æœ‰å…ƒç´ éƒ½æœ‰ page_content å±æ€§
                    if not isinstance(docs, list):
                        docs = [docs] if docs else []

                    # è¿‡æ»¤å¹¶è®°å½•é—®é¢˜æ–‡æ¡£
                    valid_docs = []
                    for i, d in enumerate(docs):
                        if hasattr(d, "page_content"):
                            valid_docs.append(d)
                        else:
                            st.warning(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file_path)} çš„ç¬¬ {i} ä¸ªæ–‡æ¡£ä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼ˆç±»å‹: {type(d)}ï¼‰ï¼Œå·²è·³è¿‡")

                    docs = valid_docs
                    if not docs:
                        st.warning(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file_path)} æ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ–‡æ¡£")
                        continue

                    all_documents.extend(docs)

                    file_stats.append({
                        "name": os.path.basename(file_path),
                        "type": "ğŸ“„ é»˜è®¤æ–‡ä»¶",
                        "chars": sum(len(d.page_content) for d in docs)
                    })
                except Exception as e:
                    st.error(f"âŒ é»˜è®¤æ–‡ä»¶ {file_path} è¯»å–å¤±è´¥: {e}")
                    continue

        # === A. å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ ===
        if uploaded_files:
            for uploaded_file in uploaded_files:
                current_item += 1
                progress = current_item / (total_items + 1)
                progress_bar.progress(progress)
                status_text.text(f"ğŸ“– Loading file: {uploaded_file.name}")

                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    temp_filepath = tmp_file.name
                
                try:
                    if uploaded_file.name.lower().endswith(".pdf"):
                        loader = PyPDFLoader(temp_filepath)
                    else:
                        loader = TextLoader(temp_filepath, encoding="utf-8")

                    docs = loader.load()

                    # ç¡®ä¿ docs æ˜¯åˆ—è¡¨ä¸”æ‰€æœ‰å…ƒç´ éƒ½æœ‰ page_content å±æ€§
                    if not isinstance(docs, list):
                        docs = [docs] if docs else []

                    # è¿‡æ»¤å¹¶è®°å½•é—®é¢˜æ–‡æ¡£
                    valid_docs = []
                    for i, d in enumerate(docs):
                        if hasattr(d, "page_content"):
                            valid_docs.append(d)
                        else:
                            st.warning(f"âš ï¸ æ–‡ä»¶ {uploaded_file.name} çš„ç¬¬ {i} ä¸ªæ–‡æ¡£ä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼ˆç±»å‹: {type(d)}ï¼‰ï¼Œå·²è·³è¿‡")

                    docs = valid_docs
                    if not docs:
                        st.warning(f"âš ï¸ æ–‡ä»¶ {uploaded_file.name} æ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ–‡æ¡£")
                        continue

                    all_documents.extend(docs)

                    file_stats.append({
                        "name": uploaded_file.name,
                        "type": "ğŸ“„ ä¸Šä¼ æ–‡ä»¶",
                        "chars": sum(len(d.page_content) for d in docs)
                    })
                except Exception as e:
                    st.error(f"âŒ æ–‡ä»¶ {uploaded_file.name} è¯»å–å¤±è´¥: {e}")
                    continue
                finally:
                    if os.path.exists(temp_filepath):
                        try:
                            os.unlink(temp_filepath)
                        except Exception:
                            pass  # é™é»˜å¤„ç†æ¸…ç†å¤±è´¥

        # === B. å¤„ç† URL (å¸¦æµè§ˆå™¨ä¼ªè£…) ===
        if urls:
            for url in urls:
                if not url.strip(): continue
                
                current_item += 1
                progress = current_item / (total_items + 1)
                progress_bar.progress(progress)
                status_text.text(f"ğŸŒ Scraping webpage: {url}")

                try:
                    # ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé˜²æ­¢ 403 é”™è¯¯
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36"
                    }
                    loader = WebBaseLoader(url, header_template=headers)
                    docs = loader.load()

                    # ç¡®ä¿ docs æ˜¯åˆ—è¡¨ä¸”æ‰€æœ‰å…ƒç´ éƒ½æœ‰ page_content å±æ€§
                    if not isinstance(docs, list):
                        docs = [docs] if docs else []

                    # è¿‡æ»¤å¹¶è®°å½•é—®é¢˜æ–‡æ¡£
                    valid_docs = []
                    for i, d in enumerate(docs):
                        if hasattr(d, "page_content"):
                            valid_docs.append(d)
                        else:
                            st.warning(f"âš ï¸ ç½‘é¡µ {url} çš„ç¬¬ {i} ä¸ªæ–‡æ¡£ä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼ˆç±»å‹: {type(d)}ï¼‰ï¼Œå·²è·³è¿‡")

                    docs = valid_docs
                    if not docs:
                        st.warning(f"âš ï¸ ç½‘é¡µ {url} æ²¡æœ‰æå–åˆ°æœ‰æ•ˆæ–‡æ¡£")
                        continue

                    all_documents.extend(docs)

                    file_stats.append({
                        "name": url,
                        "type": "ğŸŒ ç½‘é¡µ",
                        "chars": sum(len(d.page_content) for d in docs)
                    })
                except Exception as e:
                    st.warning(f"âš ï¸ ç½‘é¡µçˆ¬å–å¤±è´¥ ({url}): {e}")
                    continue

        # === C. åˆ‡åˆ†ä¸å‘é‡åŒ– ===
        if not all_documents:
            progress_bar.empty()
            status_text.empty()
            st.error("âŒ æœªæå–åˆ°æœ‰æ•ˆæ–‡æœ¬")
            return

        # æœ€åä¸€æ¬¡æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯æœ‰æ•ˆçš„
        status_text.text("ğŸ” Validating documents...")
        valid_all_documents = []
        for i, doc in enumerate(all_documents):
            if hasattr(doc, "page_content"):
                valid_all_documents.append(doc)
            else:
                st.warning(f"âš ï¸ æ£€æµ‹åˆ°ç¬¬ {i} ä¸ªæ–‡æ¡£æ ¼å¼å¼‚å¸¸ï¼ˆç±»å‹: {type(doc)}ï¼‰ï¼Œå·²è·³è¿‡")

        if not valid_all_documents:
            progress_bar.empty()
            status_text.empty()
            st.error("âŒ æ‰€æœ‰æ–‡æ¡£éƒ½ä¸æ˜¯æœ‰æ•ˆæ ¼å¼")
            return

        all_documents = valid_all_documents

        status_text.text("âœ‚ï¸ Splitting documents...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        split_docs = text_splitter.split_documents(all_documents)

        # éªŒè¯åˆ‡åˆ†åçš„æ–‡æ¡£
        status_text.text("ğŸ” Validating chunks...")
        valid_split_docs = []
        for i, doc in enumerate(split_docs):
            if hasattr(doc, "page_content"):
                valid_split_docs.append(doc)
            else:
                st.warning(f"âš ï¸ åˆ‡åˆ†åç¬¬ {i} ä¸ªæ–‡æ¡£æ ¼å¼å¼‚å¸¸ï¼ˆç±»å‹: {type(doc)}ï¼‰ï¼Œå·²è·³è¿‡")

        if not valid_split_docs:
            progress_bar.empty()
            status_text.empty()
            st.error("âŒ åˆ‡åˆ†åæ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£")
            return

        split_docs = valid_split_docs

        status_text.text("ğŸ”¢ Generating vector index (first run may download model, please wait)...")
        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        vectorstore = FAISS.from_documents(split_docs, embeddings)

        st.session_state["vectorstore"] = vectorstore
        st.session_state["doc_stats"] = file_stats 
        
        progress_bar.empty()
        status_text.empty()
        st.success(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼å…±åŒ…å« {len(file_stats)} ä¸ªæ•°æ®æºã€‚")

    except Exception as e:
        progress_bar.empty()
        status_text.empty()
        st.error(f"âŒ æ„å»ºè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        st.exception(e)  # æ˜¾ç¤ºå®Œæ•´é”™è¯¯å †æ ˆï¼Œä¾¿äºè°ƒè¯•
