"""
ä½¿ç”¨ Selenium ç»•è¿‡åçˆ¬è™«æœºåˆ¶çˆ¬å– NTU ç½‘ç«™

å®‰è£…ä¾èµ–:
pip install selenium webdriver-manager

ä½¿ç”¨æ–¹æ³•:
python scripts/scraper_selenium.py
"""

import time
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


class NTUWebScraper:
    def __init__(self, headless=True):
        """
        åˆå§‹åŒ– Selenium WebDriver

        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰
        """
        options = Options()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        # ä¼ªè£…æˆçœŸå®æµè§ˆå™¨
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

        # è‡ªåŠ¨ä¸‹è½½å¹¶ä½¿ç”¨ ChromeDriver
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)
        self.driver.implicitly_wait(10)  # éšå¼ç­‰å¾…10ç§’

    def scrape_page(self, url, output_file=None):
        """
        çˆ¬å–å•ä¸ªé¡µé¢

        Args:
            url: ç›®æ ‡URL
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            é¡µé¢æ–‡æœ¬å†…å®¹
        """
        print(f"æ­£åœ¨è®¿é—®: {url}")
        self.driver.get(url)

        # ç­‰å¾…é¡µé¢åŠ è½½ï¼ˆç­‰å¾…æŸä¸ªå…³é”®å…ƒç´ å‡ºç°ï¼‰
        try:
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except Exception as e:
            print(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
            return None

        # æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
        self._scroll_page()

        # æå–æ–‡æœ¬å†…å®¹
        try:
            # å°è¯•æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸï¼ˆæ ¹æ®NTUç½‘ç«™ç»“æ„è°ƒæ•´ï¼‰
            main_content = self.driver.find_element(By.TAG_NAME, "main")
            text = main_content.text
        except:
            # å¦‚æœæ²¡æœ‰ main æ ‡ç­¾ï¼Œå°±ç”¨ body
            text = self.driver.find_element(By.TAG_NAME, "body").text

        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(f"æ¥æº: {url}\n")
                f.write(f"æŠ“å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*80 + "\n\n")
                f.write(text)
            print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")

        return text

    def scrape_multiple(self, urls, output_dir="data/scraped"):
        """
        æ‰¹é‡çˆ¬å–å¤šä¸ªé¡µé¢

        Args:
            urls: URLåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        results = {}
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] å¤„ç†: {url}")

            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä»URLæå–ï¼‰
            filename = url.split('/')[-1] or 'index'
            filename = filename.replace('?', '_').replace('&', '_')
            output_file = f"{output_dir}/{filename}.txt"

            text = self.scrape_page(url, output_file)
            results[url] = text

            # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…è¢«ban
            time.sleep(2)

        return results

    def _scroll_page(self):
        """æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½å†…å®¹"""
        # è·å–é¡µé¢é«˜åº¦
        last_height = self.driver.execute_script("return document.body.scrollHeight")

        while True:
            # æ»šåŠ¨åˆ°åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)

            # è®¡ç®—æ–°çš„æ»šåŠ¨é«˜åº¦
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        self.driver.quit()
        print("\nâœ… æµè§ˆå™¨å·²å…³é—­")


# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    # NTU é‡è¦é¡µé¢åˆ—è¡¨
    urls = [
        "https://www.ntu.edu.sg/life-at-ntu/accommodation",
        "https://www.ntu.edu.sg/admissions/graduate/requirements",
        "https://www.ntu.edu.sg/education/academic-calendar",
        # æ·»åŠ æ›´å¤šä½ éœ€è¦çš„é¡µé¢
    ]

    scraper = NTUWebScraper(headless=True)

    try:
        print("ğŸš€ å¼€å§‹çˆ¬å–...")
        scraper.scrape_multiple(urls)
        print("\nğŸ‰ çˆ¬å–å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
    finally:
        scraper.close()
