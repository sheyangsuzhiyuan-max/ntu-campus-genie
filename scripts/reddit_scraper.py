"""
ä» Reddit r/NTU çˆ¬å–ç²¾åå†…å®¹

å®‰è£…ä¾èµ–:
pip install praw

ä½¿ç”¨æ–¹æ³•:
1. åˆ° https://www.reddit.com/prefs/apps åˆ›å»ºåº”ç”¨è·å– API credentials
2. å¡«å†™ä¸‹é¢çš„é…ç½®
3. è¿è¡Œ: python scripts/reddit_scraper.py
"""

import praw
from datetime import datetime
from pathlib import Path


class RedditNTUScraper:
    def __init__(self, client_id, client_secret, user_agent):
        """
        åˆå§‹åŒ– Reddit API å®¢æˆ·ç«¯

        Args:
            client_id: Reddit App Client ID
            client_secret: Reddit App Secret
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
        """
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
        self.subreddit = self.reddit.subreddit("NTU")

    def scrape_top_posts(self, limit=50, time_filter="all", min_score=10):
        """
        çˆ¬å–çƒ­é—¨å¸–å­

        Args:
            limit: æ•°é‡é™åˆ¶
            time_filter: æ—¶é—´èŒƒå›´ ("all", "year", "month", "week")
            min_score: æœ€ä½ç‚¹èµæ•°

        Returns:
            å¸–å­åˆ—è¡¨
        """
        posts = []
        for submission in self.subreddit.top(time_filter=time_filter, limit=limit):
            if submission.score >= min_score:
                posts.append({
                    'title': submission.title,
                    'score': submission.score,
                    'text': submission.selftext,
                    'url': submission.url,
                    'num_comments': submission.num_comments,
                    'created': datetime.fromtimestamp(submission.created_utc),
                })
        return posts

    def search_posts(self, query, limit=30):
        """
        æœç´¢ç‰¹å®šå…³é”®è¯çš„å¸–å­

        Args:
            query: æœç´¢å…³é”®è¯
            limit: æ•°é‡é™åˆ¶

        Returns:
            å¸–å­åˆ—è¡¨
        """
        posts = []
        for submission in self.subreddit.search(query, limit=limit):
            posts.append({
                'title': submission.title,
                'score': submission.score,
                'text': submission.selftext,
                'url': submission.url,
                'num_comments': submission.num_comments,
            })
        return posts

    def save_to_file(self, posts, output_file):
        """
        ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶

        Args:
            posts: å¸–å­åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"Reddit r/NTU å†…å®¹æ±‡æ€»\n")
            f.write(f"æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å…± {len(posts)} æ¡å¸–å­\n")
            f.write("="*80 + "\n\n")

            for i, post in enumerate(posts, 1):
                f.write(f"ã€å¸–å­ {i}ã€‘{post['title']}\n")
                f.write(f"è¯„åˆ†: {post['score']} | è¯„è®ºæ•°: {post['num_comments']}\n")
                if 'created' in post:
                    f.write(f"å‘å¸ƒæ—¶é—´: {post['created']}\n")
                f.write(f"é“¾æ¥: {post['url']}\n")
                f.write("-"*80 + "\n")

                if post['text']:
                    f.write(post['text'])
                    f.write("\n")
                else:
                    f.write("[æ­¤å¸–å­æ— æ­£æ–‡å†…å®¹ï¼Œå¯èƒ½æ˜¯é“¾æ¥è´´]\n")

                f.write("\n" + "="*80 + "\n\n")

        print(f"âœ… å·²ä¿å­˜ {len(posts)} æ¡å¸–å­åˆ°: {output_file}")


# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    # âš ï¸ éœ€è¦å…ˆåˆ° https://www.reddit.com/prefs/apps åˆ›å»ºåº”ç”¨
    # å¡«å†™ä½ çš„ credentials
    CLIENT_ID = "YOUR_CLIENT_ID"  # æ›¿æ¢ä¸ºä½ çš„
    CLIENT_SECRET = "YOUR_CLIENT_SECRET"  # æ›¿æ¢ä¸ºä½ çš„
    USER_AGENT = "NTU_Genie_Scraper/1.0"

    # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œè·³è¿‡
    if CLIENT_ID == "YOUR_CLIENT_ID":
        print("âš ï¸ è¯·å…ˆé…ç½® Reddit API credentials")
        print("è®¿é—®: https://www.reddit.com/prefs/apps")
        exit()

    scraper = RedditNTUScraper(CLIENT_ID, CLIENT_SECRET, USER_AGENT)

    # æ–¹æ¡ˆ1: çˆ¬å–çƒ­é—¨å¸–å­
    print("ğŸ” æ­£åœ¨è·å– r/NTU çƒ­é—¨å¸–å­...")
    top_posts = scraper.scrape_top_posts(limit=50, min_score=10)
    scraper.save_to_file(top_posts, "data/reddit_ntu_top.txt")

    # æ–¹æ¡ˆ2: æœç´¢ç‰¹å®šè¯é¢˜
    topics = [
        "accommodation",
        "housing",
        "student pass",
        "visa",
        "orientation",
    ]

    for topic in topics:
        print(f"\nğŸ” æœç´¢è¯é¢˜: {topic}")
        posts = scraper.search_posts(topic, limit=20)
        if posts:
            scraper.save_to_file(posts, f"data/reddit_ntu_{topic}.txt")

    print("\nğŸ‰ Reddit å†…å®¹çˆ¬å–å®Œæˆï¼")
