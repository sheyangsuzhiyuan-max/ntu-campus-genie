import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate

# âœ… æ”¹å›æ ‡å‡†å†™æ³• (ç¯å¢ƒä¿®å¥½åï¼Œè¿™ä¸ªæ‰æ˜¯å¯¹çš„)
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain


def run_chat(deepseek_api_key: str) -> None:
    """ä¸»èŠå¤©é€»è¾‘ï¼šæ ¹æ®æ˜¯å¦æœ‰ vectorstore å†³å®šèµ° RAG è¿˜æ˜¯æ™®é€šå¯¹è¯ã€‚"""
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {
                "role": "assistant",
                "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI åŠ©æ‰‹ã€‚è¯·ä¸Šä¼ æ–‡æ¡£å¹¶ç‚¹å‡»æ„å»ºçŸ¥è¯†åº“ï¼Œç„¶åé—®æˆ‘ç›¸å…³é—®é¢˜ã€‚",
            }
        ]

    # å±•ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt := st.chat_input("è¯·è¾“å…¥é—®é¢˜..."):
        if not deepseek_api_key:
            st.info("è¯·å…ˆè®¾ç½® API Keyã€‚")
            st.stop()

        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        try:
            # DeepSeek æ¨¡å‹é…ç½®
            llm = ChatOpenAI(
                model="deepseek-chat",
                openai_api_key=deepseek_api_key,
                base_url="https://api.deepseek.com/v1",
            )

            # RAG æ¨¡å¼åˆ¤æ–­
            if "vectorstore" in st.session_state:
                vectorstore = st.session_state["vectorstore"]
                retriever = vectorstore.as_retriever(search_kwargs={"k": 6})

                # ... (å‰æ–‡ä»£ç ä¸å˜) ...

                # --- ä¿®æ”¹å¼€å§‹ï¼šä¼˜åŒ– Prompt ---
                prompt_tmpl = ChatPromptTemplate.from_template(
                    """
                    ä½ æ˜¯ä¸€ä¸ªçƒ­å¿ƒã€ä¸“ä¸šçš„ NTU æ ¡å›­åŠ©æ‰‹ï¼ˆå­¦é•¿/å­¦å§é£æ ¼ï¼‰ã€‚
                    ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©æ–°åŒå­¦è§£ç­”å…³äºç­¾è¯ã€å®¿èˆã€é€‰è¯¾ç­‰æ–¹é¢çš„é—®é¢˜ã€‚

                    è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹ã€èƒŒæ™¯ä¿¡æ¯ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚

                    å›ç­”è¦æ±‚ï¼š
                    1. **è¯­æ°”äº²åˆ‡**ï¼šå¯ä»¥ä½¿ç”¨ emoji (å¦‚ ğŸ , ğŸ“…, ğŸ’¡) æ¥ç¼“è§£ç”¨æˆ·çš„ç„¦è™‘ã€‚
                    2. **ç»“æ„æ¸…æ™°**ï¼šå¦‚æœç­”æ¡ˆåŒ…å«æ­¥éª¤æˆ–å¤šä¸ªé€‰é¡¹ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ Markdown åˆ—è¡¨ï¼ˆ- æˆ– 1.ï¼‰åˆ—å‡ºã€‚
                    3. **å¼•ç”¨æ¥æº**ï¼šå¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸­æä¾›äº†é“¾æ¥æˆ–æ¥æºï¼Œè¯·åœ¨å›ç­”æœ«å°¾é™„ä¸Šã€‚
                    4. **è¯šå®å®ˆä¿¡**ï¼šå¦‚æœã€èƒŒæ™¯ä¿¡æ¯ã€‘é‡Œæ²¡æœ‰æåˆ°çš„å†…å®¹ï¼Œè¯·ç›´æ¥è¯´â€œæŠ±æ­‰ï¼Œå½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äºæ­¤é—®é¢˜çš„å…·ä½“ä¿¡æ¯â€ï¼Œå»ºè®®ç”¨æˆ·å’¨è¯¢å­¦æ ¡ One Stop ä¸­å¿ƒã€‚ä¸è¦ç¼–é€ æ—¥æœŸæˆ–ä»·æ ¼ã€‚

                    ã€èƒŒæ™¯ä¿¡æ¯ã€‘ï¼š
                    {context}

                    ã€é—®é¢˜ã€‘ï¼š
                    {input}
                    """
                )
                # --- ä¿®æ”¹ç»“æŸ ---
                
                doc_chain = create_stuff_documents_chain(llm, prompt_tmpl)
                # ... (åæ–‡ä»£ç ä¸å˜) ...

                doc_chain = create_stuff_documents_chain(llm, prompt_tmpl)
                rag_chain = create_retrieval_chain(retriever, doc_chain)

                with st.chat_message("assistant"):
                    st.caption("ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...")
                    response = rag_chain.invoke({"input": prompt})
                    answer = response["answer"]

                    # Debugï¼šå±•ç¤ºå‚è€ƒæ–‡æ¡£ç‰‡æ®µ
                    with st.expander("ğŸ•µï¸â€â™‚ï¸ Debug: AI å‚è€ƒäº†å“ªäº›æ–‡æ¡£ç‰‡æ®µï¼Ÿ"):
                        retrieved_docs = retriever.invoke(prompt)
                        for i, doc in enumerate(retrieved_docs):
                            st.markdown(
                                f"**ç‰‡æ®µ {i+1} (æ¥è‡ª {doc.metadata.get('source', 'æœªçŸ¥')}):**"
                            )
                            st.text(doc.page_content)
                            st.divider()

                    st.write(answer)
            else:
                # æ™®é€šæ¨¡å¼
                with st.chat_message("assistant"):
                    response = llm.invoke([HumanMessage(content=prompt)])
                    answer = response.content
                    st.write(answer)

            st.session_state.messages.append(
                {"role": "assistant", "content": answer}
            )

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")



