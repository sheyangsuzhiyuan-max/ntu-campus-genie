import streamlit as st

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate

from config import (
    DEEPSEEK_MODEL,
    DEEPSEEK_BASE_URL,
    DEFAULT_RETRIEVAL_K,
    SYSTEM_PROMPT_CHAT,
    SYSTEM_PROMPT_HOUSING,
    USE_RERANK,
    RERANK_TOP_K,
)
from utils import log_feedback, get_unique_button_key, init_session_state

# åŸæœ¬ç›´æ¥ä» langchain.chains import create_retrieval_chain ä¼šåœ¨éƒ¨åˆ†ç‰ˆæœ¬ä¸­æŠ¥é”™
# æˆ‘ä»¬ç”¨å…¼å®¹ shimï¼šè‹¥åŸå‡½æ•°å­˜åœ¨å°±ç”¨åŸå‡½æ•°ï¼Œå¦åˆ™å®šä¹‰ä¸€ä¸ªä¸åŸç”¨æ³•å…¼å®¹çš„å®ç°
try:
    from langchain.chains import create_retrieval_chain  # type: ignore
except Exception:
    # å…¼å®¹å®ç°ï¼šæ¥æ”¶ (retriever, doc_chain) å¹¶è¿”å›æœ‰ .invoke(inputs) çš„å¯¹è±¡
    from langchain.chains.combine_documents import create_stuff_documents_chain  # ä¿ç•™å¼•ç”¨ä½ç½®ï¼ˆä½ çš„ä»£ç ä¹Ÿå•ç‹¬ import äº†ï¼‰
    import logging

    logging.info("create_retrieval_chain not found â€” using local shim")

    def create_retrieval_chain(retriever, doc_chain, **kwargs):
        """
        Shim for older/newer langchain API differences.

        Expected call in your code:
            rag_chain = create_retrieval_chain(retriever, doc_chain)
            response = rag_chain.invoke({"input": prompt})
            answer = response["answer"]

        This shim:
          - uses retriever.get_relevant_documents(query) (or falls back to other call patterns)
          - calls doc_chain with {"input_documents": docs, "input": query}
          - attempts to normalize the result and returns {"answer": text}
        """
        class SimpleRAG:
            def __init__(self, retriever, doc_chain):
                self.retriever = retriever
                self.doc_chain = doc_chain

            def _get_documents(self, query):
                # Common getter names across vectorstores/retrievers
                raw_docs = []
                if hasattr(self.retriever, "get_relevant_documents"):
                    raw_docs = self.retriever.get_relevant_documents(query)
                elif hasattr(self.retriever, "get_relevant_source_documents"):
                    raw_docs = self.retriever.get_relevant_source_documents(query)
                else:
                    # some retrievers are callable
                    try:
                        raw_docs = self.retriever(query)
                    except Exception:
                        raw_docs = []

                # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨
                if not isinstance(raw_docs, list):
                    raw_docs = [raw_docs] if raw_docs else []

                # è¿‡æ»¤æ‰é Document å¯¹è±¡ï¼Œåªä¿ç•™æœ‰ page_content å±æ€§çš„å¯¹è±¡
                return [d for d in raw_docs if hasattr(d, "page_content")]

            def _call_doc_chain(self, docs, query):
                inputs = {"input_documents": docs, "input": query}
                # prefer invoke
                if hasattr(self.doc_chain, "invoke"):
                    try:
                        return self.doc_chain.invoke(inputs)
                    except TypeError:
                        # some chains expect positional args or different signature â€” try other options
                        pass
                # try run
                if hasattr(self.doc_chain, "run"):
                    try:
                        return self.doc_chain.run(input_documents=docs, input=query)
                    except TypeError:
                        try:
                            return self.doc_chain.run(query)
                        except Exception:
                            pass
                # if callable, call with inputs
                if callable(self.doc_chain):
                    try:
                        return self.doc_chain(inputs)
                    except Exception:
                        pass
                # fallback: return joined docs text
                try:
                    # ç¡®ä¿ docs éƒ½æœ‰ page_content å±æ€§
                    valid_docs = [d for d in docs if hasattr(d, "page_content")]
                    joined = "\n\n".join(d.page_content for d in valid_docs)
                    return {"output_text": joined}
                except Exception:
                    return {"output_text": ""}

            def _normalize_to_text(self, result):
                # result can be dict, str, or other
                if isinstance(result, str):
                    return result
                if isinstance(result, dict):
                    for k in ("answer", "output_text", "text", "output"):
                        if k in result:
                            # if nested structures, coerce to str
                            val = result[k]
                            return val if isinstance(val, str) else str(val)
                    # fallback: first string value
                    for v in result.values():
                        if isinstance(v, str):
                            return v
                    return str(result)
                return str(result)

            def invoke(self, inputs: dict):
                # expects inputs like {"input": "user question"}
                query = inputs.get("input") or inputs.get("query") or ""
                docs = self._get_documents(query) or []

                # å†æ¬¡ç¡®ä¿æ‰€æœ‰ docs éƒ½æœ‰ page_contentï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
                docs = [d for d in docs if hasattr(d, "page_content")]

                result = self._call_doc_chain(docs, query)
                text = self._normalize_to_text(result)
                return {"answer": text, "source_documents": docs}

        return SimpleRAG(retriever, doc_chain)

# ä½ çš„åŸæœ‰ importï¼ˆä½ ä¹‹å‰æ–‡ä»¶é‡Œä¹Ÿæœ‰è¿™è¡Œï¼‰
from langchain.chains.combine_documents import create_stuff_documents_chain


def rerank_documents(query: str, documents: list, top_k: int = 3):
    """
    ä½¿ç”¨ FlashRank å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’åº

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        documents: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        top_k: ä¿ç•™å‰ k ä¸ªæ–‡æ¡£

    Returns:
        é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
    """
    # ç¡®ä¿ documents æ˜¯åˆ—è¡¨
    if not isinstance(documents, list):
        documents = [documents] if documents else []

    # è¿‡æ»¤æ‰æ²¡æœ‰ page_content çš„æ¡ç›®ï¼Œé¿å…åç»­ AttributeError
    clean_docs = [d for d in documents if hasattr(d, "page_content") and hasattr(d, "metadata")]
    if not clean_docs:
        return []

    try:
        from flashrank import Ranker, RerankRequest

        ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="/tmp")

        passages = []
        for i, doc in enumerate(clean_docs):
            passages.append(
                {
                    "id": i,
                    "text": getattr(doc, "page_content", ""),
                    "meta": getattr(doc, "metadata", {}),
                }
            )

        rerank_request = RerankRequest(query=query, passages=passages)
        results = ranker.rerank(rerank_request)

        reranked_docs = []
        for result in results[:top_k]:
            doc_id = result["id"]
            if 0 <= doc_id < len(clean_docs):
                reranked_docs.append(clean_docs[doc_id])

        return reranked_docs
    except Exception as e:
        # å¦‚æœ rerank å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æ¡£
        st.warning(f"âš ï¸ Rerank å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ: {e}")
        return clean_docs[:top_k]


def run_chat(deepseek_api_key: str) -> None:
    # 1. åˆå§‹åŒ–ä¼šè¯ & æœ€è¿‘ä¸€æ¬¡äº¤äº’
    init_session_state()

    # 2. å±•ç¤ºå†å²æ¶ˆæ¯ï¼ˆå¸¦åé¦ˆæŒ‰é’®ï¼‰
    for idx, msg in enumerate(st.session_state.messages):
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

            # ä¸ºæ¯æ¡åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åé¦ˆæŒ‰é’®ï¼ˆè·³è¿‡æ¬¢è¿æ¶ˆæ¯ï¼‰
            if msg["role"] == "assistant" and idx > 0:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰åé¦ˆè®°å½•
                feedback_key = f"feedback_{idx}"
                if feedback_key not in st.session_state:
                    st.session_state[feedback_key] = None

                # å¦‚æœè¿˜æ²¡æœ‰åé¦ˆï¼Œæ˜¾ç¤ºæŒ‰é’®
                if st.session_state[feedback_key] is None:
                    fb_col1, fb_col2 = st.columns(2)
                    with fb_col1:
                        if st.button("ğŸ‘ Helpful", key=f"fb_up_{idx}"):
                            # ä»æ¶ˆæ¯ä¸­æå–é—®ç­”ä¿¡æ¯
                            question = st.session_state.messages[idx - 1]["content"] if idx > 0 else ""
                            answer = msg["content"]
                            interaction = {
                                "question": question,
                                "answer": answer,
                                "used_rag": msg.get("used_rag", False),
                                "sources": msg.get("sources", []),
                            }
                            if log_feedback("up", interaction):
                                st.session_state[feedback_key] = "up"
                                st.toast("Thank you for your feedback!", icon="ğŸ‘")
                                st.rerun()
                    with fb_col2:
                        if st.button("ğŸ‘ Not Helpful", key=f"fb_down_{idx}"):
                            question = st.session_state.messages[idx - 1]["content"] if idx > 0 else ""
                            answer = msg["content"]
                            interaction = {
                                "question": question,
                                "answer": answer,
                                "used_rag": msg.get("used_rag", False),
                                "sources": msg.get("sources", []),
                            }
                            if log_feedback("down", interaction):
                                st.session_state[feedback_key] = "down"
                                st.toast("Feedback recorded!", icon="ğŸ‘")
                                st.rerun()
                else:
                    # å·²ç»æœ‰åé¦ˆï¼Œæ˜¾ç¤ºçŠ¶æ€
                    if st.session_state[feedback_key] == "up":
                        st.caption("âœ… You found this helpful")
                    else:
                        st.caption("âœ… Feedback recorded")

    # 3. Support "prefilled questions" (from quick start buttons) + manual input
    user_input = st.chat_input("Type your question here...")
    prompt = None

    # If "quick start" button was clicked, use prefill first
    prefill = st.session_state.get("prefill")
    if prefill:
        prompt = prefill
        st.session_state["prefill"] = ""
    elif user_input:
        prompt = user_input

    # Return if no input
    if not prompt:
        return

    # 4. Prompt and stop if no API Key
    if not deepseek_api_key:
        st.info("Please enter your DeepSeek API Key in the sidebar first.")
        st.stop()

    # 5. æŠŠæœ¬è½®ç”¨æˆ·æ¶ˆæ¯åŠ å…¥å†å²å¹¶å±•ç¤º
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    try:
        # åˆå§‹åŒ– LLM
        llm = ChatOpenAI(
            model=DEEPSEEK_MODEL,
            openai_api_key=deepseek_api_key,
            base_url=DEEPSEEK_BASE_URL,
        )

        used_rag = False
        source_names = []

        # 6. å¦‚æœå·²ç»æœ‰å‘é‡çŸ¥è¯†åº“ â†’ ä½¿ç”¨ RAG
        if "vectorstore" in st.session_state:
            vectorstore = st.session_state["vectorstore"]

            with st.chat_message("assistant"):
                st.caption("ğŸ” Searching knowledge base...")

                # Retrieve documents first
                retriever = vectorstore.as_retriever(search_kwargs={"k": DEFAULT_RETRIEVAL_K})
                raw_docs = retriever.get_relevant_documents(prompt)

                # Ensure return value is a list
                if not isinstance(raw_docs, list):
                    raw_docs = [raw_docs] if raw_docs else []

                # Filter out non-Document objects to avoid missing attributes
                retrieved_docs = [
                    d for d in raw_docs if hasattr(d, "page_content") and hasattr(d, "metadata")
                ]

                # If rerank is enabled, reorder the retrieved documents
                if USE_RERANK and len(retrieved_docs) > 0:
                    st.caption("ğŸ¯ Optimizing results (Rerank)...")
                    docs = rerank_documents(prompt, retrieved_docs, top_k=RERANK_TOP_K)
                else:
                    docs = retrieved_docs[:RERANK_TOP_K]

                # æœ€ç»ˆå†é˜²å¾¡ä¸€æ¬¡ï¼šåªä¿ç•™å…·å¤‡ page_content çš„æ–‡æ¡£
                docs = [d for d in docs if hasattr(d, "page_content")]

                # ä½¿ç”¨ rerank åçš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
                prompt_tmpl = ChatPromptTemplate.from_template(SYSTEM_PROMPT_CHAT)
                doc_chain = create_stuff_documents_chain(llm, prompt_tmpl)

                # âœ… ä¿®å¤ï¼šdoc_chain æœŸæœ›çš„æ˜¯ Document å¯¹è±¡åˆ—è¡¨ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
                # æ­£ç¡®çš„è°ƒç”¨æ–¹å¼æ˜¯ä¼ é€’ Document å¯¹è±¡åˆ—è¡¨
                result = doc_chain.invoke({"context": docs, "input": prompt})

                if isinstance(result, dict):
                    answer = result.get("output_text") or result.get("answer") or str(result)
                else:
                    answer = str(result)

                # æ•´ç†æ¥æºï¼ˆé˜²å¾¡ï¼šç¡®ä¿æœ‰ metadataï¼‰
                seen = set()
                for d in docs:
                    src = None
                    meta = getattr(d, "metadata", {}) or {}
                    for key in ("source", "file_path", "url"):
                        if meta.get(key):
                            src = meta[key]
                            break
                    if not src:
                        src = "Unknown source"
                    if src not in seen:
                        seen.add(src)
                        source_names.append(src)

                st.write(answer)

                if source_names:
                    with st.expander("ğŸ“ Reference Sources", expanded=False):
                        for name in source_names:
                            st.caption(f"- {name}")

                # æ–°å›ç­”çš„åé¦ˆæŒ‰é’®ï¼ˆç«‹å³æ˜¾ç¤ºï¼‰
                fb_col1, fb_col2 = st.columns(2)
                with fb_col1:
                    if st.button("ğŸ‘ Helpful", key=f"fb_up_new_{len(st.session_state.messages)}"):
                        interaction = {
                            "question": prompt,
                            "answer": answer,
                            "used_rag": True,
                            "sources": source_names,
                        }
                        if log_feedback("up", interaction):
                            # ä¿å­˜åé¦ˆçŠ¶æ€
                            next_idx = len(st.session_state.messages) + 1
                            st.session_state[f"feedback_{next_idx}"] = "up"
                            st.toast("Thank you for your feedback!", icon="ğŸ‘")
                            st.rerun()
                with fb_col2:
                    if st.button("ğŸ‘ Not Helpful", key=f"fb_down_new_{len(st.session_state.messages)}"):
                        interaction = {
                            "question": prompt,
                            "answer": answer,
                            "used_rag": True,
                            "sources": source_names,
                        }
                        if log_feedback("down", interaction):
                            next_idx = len(st.session_state.messages) + 1
                            st.session_state[f"feedback_{next_idx}"] = "down"
                            st.toast("Feedback recorded!", icon="ğŸ‘")
                            st.rerun()

            used_rag = True

        # 7. If no knowledge base, fallback to general chat
        else:
            with st.chat_message("assistant"):
                response = llm.invoke([HumanMessage(content=prompt)])
                answer = response.content
                st.write(answer)

                # æ–°å›ç­”çš„åé¦ˆæŒ‰é’®ï¼ˆç«‹å³æ˜¾ç¤º - éRAGæ¨¡å¼ï¼‰
                fb_col1, fb_col2 = st.columns(2)
                with fb_col1:
                    if st.button("ğŸ‘ Helpful", key=f"fb_up_new_{len(st.session_state.messages)}"):
                        interaction = {
                            "question": prompt,
                            "answer": answer,
                            "used_rag": False,
                            "sources": [],
                        }
                        if log_feedback("up", interaction):
                            next_idx = len(st.session_state.messages) + 1
                            st.session_state[f"feedback_{next_idx}"] = "up"
                            st.toast("Thank you for your feedback!", icon="ğŸ‘")
                            st.rerun()
                with fb_col2:
                    if st.button("ğŸ‘ Not Helpful", key=f"fb_down_new_{len(st.session_state.messages)}"):
                        interaction = {
                            "question": prompt,
                            "answer": answer,
                            "used_rag": False,
                            "sources": [],
                        }
                        if log_feedback("down", interaction):
                            next_idx = len(st.session_state.messages) + 1
                            st.session_state[f"feedback_{next_idx}"] = "down"
                            st.toast("Feedback recorded!", icon="ğŸ‘")
                            st.rerun()

        # 8. Add assistant response to historyï¼ˆä¿å­˜ used_rag å’Œ sources ä¿¡æ¯ï¼‰
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "used_rag": used_rag,
            "sources": source_names,
        })

        # Record last interaction for feedback (ä¿ç•™ä»¥ä¾¿å…¼å®¹å…¶ä»–å¯èƒ½çš„ç”¨é€”)
        st.session_state["last_interaction"] = {
            "question": prompt,
            "answer": answer,
            "used_rag": used_rag,
            "sources": source_names,
        }

        # æ³¨æ„ï¼šåé¦ˆæŒ‰é’®ç°åœ¨åœ¨å†å²æ¶ˆæ¯å±•ç¤ºéƒ¨åˆ†ï¼ˆç¬¬193-242è¡Œï¼‰ï¼Œæ¯æ¡æ¶ˆæ¯éƒ½æœ‰ç‹¬ç«‹çš„æŒ‰é’®

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        st.error(f"Error occurred: {e}")
        with st.expander("ğŸ› View Detailed Error (Debug)"):
            st.code(error_details)


def generate_housing_plan(preferences: dict, deepseek_api_key: str) -> str:
    """
    Generate housing recommendations based on user preferences and knowledge base.
    preferences example:
    {
        "budget": "Budget-friendly",
        "privacy": "Very important",
        "stay_term": "Full academic year (2 semesters)",
    }
    """
    if "vectorstore" not in st.session_state:
        return "No housing knowledge base found. Please upload documents or enter NTU webpage URLs to build the knowledge base first."

    if not deepseek_api_key:
        return "DeepSeek API Key not set. Please enter it in the sidebar first."

    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        model=DEEPSEEK_MODEL,
        openai_api_key=deepseek_api_key,
        base_url=DEEPSEEK_BASE_URL,
    )

    vectorstore = st.session_state["vectorstore"]
    retriever = vectorstore.as_retriever(search_kwargs={"k": DEFAULT_RETRIEVAL_K})

    # æŠŠåå¥½è½¬æˆä¸€æ®µè‡ªç„¶è¯­è¨€æè¿°ï¼Œä½œä¸ºæ£€ç´¢æŸ¥è¯¢
    pref_text = (
        f"é¢„ç®—å€¾å‘ï¼š{preferences.get('budget')}\n"
        f"éšç§ / ç‹¬ç«‹å«ç”Ÿé—´ï¼š{preferences.get('privacy')}\n"
        f"é¢„è®¡å…¥ä½æ—¶é•¿ï¼š{preferences.get('stay_term')}\n"
    )

    # ä½¿ç”¨å¸¦åå¥½ä¿¡æ¯çš„ prompt æ¨¡æ¿ - æ³¨æ„ï¼šSYSTEM_PROMPT_HOUSING éœ€è¦ preferences, context, input ä¸‰ä¸ªå‚æ•°
    # ä½† create_stuff_documents_chain åªæ”¯æŒ context å’Œ inputï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç†

    # Solution: Embed preferences into input, use simplified prompt
    simplified_prompt = """
You are an expert assistant familiar with NTU graduate housing.
Below are a student's housing preferences. Please provide recommendations based on the [Context Information].

Required output structure (respond in both English and Chinese):
1. Summarize their needs in 2-3 sentences (both in English and Chinese)
2. Recommend 1-2 specific housing options (e.g., Graduate Hall 1 twin sharing / North Hill single room),
   explaining why they are suitable (considering price, room type, private bathroom, etc.)
3. Provide a clear application checklist with bullet points, including:
   - When to submit the application in the system
   - Fees to pay (if mentioned in documents)
   - Important dates to check for housing results
If certain details are not mentioned in the documents, please clearly state "Not mentioned in the documents".

Please respond in both English and Chinese (ä¸­è‹±åŒè¯­å›ç­”).

[Context Information]:
{context}

[Question]:
{input}
"""

    try:
        prompt_tmpl = ChatPromptTemplate.from_template(simplified_prompt)
        doc_chain = create_stuff_documents_chain(llm, prompt_tmpl)
        rag_chain = create_retrieval_chain(retriever, doc_chain)

        # Pass preferences as input
        query = f"Based on the following preferences, recommend suitable housing:\n{pref_text}\nPlease provide a detailed housing recommendation plan."
        result = rag_chain.invoke({"input": query})
        answer = result.get("answer") or "Failed to generate recommendations. Please try again."

        return answer
    except Exception as e:
        import traceback
        error_msg = f"Error generating housing recommendations: {e}\n\nDetails:\n{traceback.format_exc()}"
        return error_msg
